{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langgraph langchain langchain-google-genai pydantic langchain-community wikipedia grandalf\n",
        "\n",
        "import os\n",
        "import operator\n",
        "from typing import Annotated, Sequence, TypedDict, List, Union, Literal\n",
        "\n",
        "# LangChain / LangGraph Imports\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# Please set your Google API Key here\n",
        "from google.colab import userdata\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "# Initialize our LLM (The Brain)\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
        "\n",
        "print(\"‚úÖ System initialized. LLM ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RyQnHCyEv1r",
        "outputId": "e03621c1-5259-471a-d230-f399311c626c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ System initialized. LLM ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Core Foundations üèóÔ∏è\n",
        "\n",
        "### **Theory: Chain vs. Graph**\n",
        "\n",
        "In traditional **Chains** (e.g., `LCEL`), execution is a Directed Acyclic Graph (DAG)‚Äîusually a straight line.\n",
        "\n",
        "\n",
        "However, real-world tasks require **Cycles** and **State**. If Step B fails, we might want to go back to Step A. If the data is ambiguous, we might want to loop until it is clarified.\n",
        "\n",
        "**LangGraph** introduces a graph structure where:\n",
        "\n",
        "1. **Nodes:** Agents or functions that perform work.\n",
        "2. **Edges:** Control flow rules (go to next node, or branch based on condition).\n",
        "3. **State:** A shared data structure accessed and updated by nodes.\n",
        "\n",
        "### **The \"Hello World\" of Graphs**\n",
        "\n",
        "Let's build a simple linear graph to understand the syntax.\n"
      ],
      "metadata": {
        "id": "zgBhhocXGjBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define the State (The memory of our graph)\n",
        "class SimpleState(TypedDict):\n",
        "    task: str\n",
        "    result: str\n",
        "\n",
        "# 2. Define Nodes (The workers)\n",
        "def research_node(state: SimpleState):\n",
        "    print(f\"üîé Researching: {state['task']}\")\n",
        "    # Simulating work\n",
        "    return {\"result\": f\"Found technical docs for {state['task']}\"}\n",
        "\n",
        "def format_node(state: SimpleState):\n",
        "    print(\"üìù Formatting report...\")\n",
        "    return {\"result\": f\"{state['result']} -> Formatted as Markdown.\"}\n",
        "\n",
        "# 3. Build the Graph\n",
        "workflow = StateGraph(SimpleState)\n",
        "\n",
        "# Add nodes\n",
        "workflow.add_node(\"researcher\", research_node)\n",
        "workflow.add_node(\"formatter\", format_node)\n",
        "\n",
        "# Add edges (Linear flow: Start -> Researcher -> Formatter -> End)\n",
        "workflow.add_edge(START, \"researcher\")\n",
        "workflow.add_edge(\"researcher\", \"formatter\")\n",
        "workflow.add_edge(\"formatter\", END)\n",
        "\n",
        "# 4. Compile\n",
        "app = workflow.compile()\n",
        "\n",
        "# 5. Execute\n",
        "output = app.invoke({\"task\": \"LangGraph Basics\"})\n",
        "print(f\"\\nüöÄ Final Output: {output['result']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeIaWtuuE1I-",
        "outputId": "6b183516-3e29-4f4d-ca71-163ef41853c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîé Researching: LangGraph Basics\n",
            "üìù Formatting report...\n",
            "\n",
            "üöÄ Final Output: Found technical docs for LangGraph Basics -> Formatted as Markdown.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: State Management üß†\n",
        "\n",
        "### **Theory: The Shared Brain**\n",
        "\n",
        "The `State` is the most critical concept in LangGraph. Unlike chains where data is passed like a hot potato, in LangGraph, nodes **read from** and **write to** a shared schema.\n",
        "\n",
        "* **Immutable Updates:** By default, when a node returns `{\"key\": \"value\"}`, it overwrites the existing key.\n",
        "* **Reducers (Annotated):** Sometimes we want to *append* data (like a chat history) rather than overwrite it. We use `Annotated` with an operator (usually `operator.add`) to achieve this.\n",
        "\n",
        "Let's upgrade our Research Assistant's brain to handle chat history and multiple search results.\n"
      ],
      "metadata": {
        "id": "viOZ6zQkGuWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a Robust State for our Assistant\n",
        "class ResearchState(TypedDict):\n",
        "    # 'operator.add' ensures that when a node returns new messages,\n",
        "    # they are APPENDED to the list, not overwriting it.\n",
        "    messages: Annotated[list[BaseMessage], operator.add]\n",
        "\n",
        "    # These fields will be overwritten by the latest update\n",
        "    query: str\n",
        "    documents: List[str]\n",
        "    draft_report: str\n",
        "    critique_count: int\n",
        "\n",
        "print(\"‚úÖ ResearchState defined with append-only message history.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26hzavfrE5kC",
        "outputId": "246718f1-b2bb-45f7-a067-a751937727df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ ResearchState defined with append-only message history.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Part 3: Nodes & Execution üèÉ‚Äç‚ôÇÔ∏è\n",
        "\n",
        "### **Theory: Nodes as Functions**\n",
        "\n",
        "A Node is simply a Python function that:\n",
        "\n",
        "1. **Receives** the current `State`.\n",
        "2. **Performs** logic (calling an LLM, searching, calculating).\n",
        "3. **Returns** a dictionary of updates to apply to the `State`.\n",
        "\n",
        "We will create two functional nodes for our assistant:\n",
        "\n",
        "1. **Search Node:** Simulates fetching data (to keep this notebook robust without external API dependencies, we will mock the search logic, but it uses real data structures).\n",
        "2. **Curator Node:** An LLM that summarizes the search results.\n"
      ],
      "metadata": {
        "id": "HUO1TZj-G5yD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_node(state: ResearchState):\n",
        "    \"\"\"\n",
        "    Simulates a search engine. In production, this would call Tavily or Google Search.\n",
        "    \"\"\"\n",
        "    query = state[\"query\"]\n",
        "    print(f\"üåê Search Agent: Searching for '{query}'...\")\n",
        "\n",
        "    # Simulated search results based on query context\n",
        "    simulated_docs = [\n",
        "        f\"Doc A: Technical overview of {query}.\",\n",
        "        f\"Doc B: Implementation details for {query} using Python.\",\n",
        "        f\"Doc C: Common pitfalls when using {query}.\"\n",
        "    ]\n",
        "\n",
        "    # We return ONLY the field we want to update\n",
        "    return {\"documents\": simulated_docs}\n",
        "\n",
        "def curator_node(state: ResearchState):\n",
        "    \"\"\"\n",
        "    Uses the LLM to summarize the found documents.\n",
        "    \"\"\"\n",
        "    print(\"üß† Curator Agent: Synthesizing information...\")\n",
        "    docs = \"\\n\".join(state[\"documents\"])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are a senior technical writer.\n",
        "    Summarize the following documents regarding '{state['query']}':\n",
        "\n",
        "    {docs}\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "\n",
        "    # Update the draft_report and add the AI message to history\n",
        "    return {\n",
        "        \"draft_report\": response.content,\n",
        "        \"messages\": [response]\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Nodes defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "om2iiKmTFNgZ",
        "outputId": "eaaf56b1-5896-4282-ec89-e91b3556a09f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Nodes defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Edges & Control Flow üîÄ\n",
        "\n",
        "### **Theory: Conditional Edges (Routers)**\n",
        "\n",
        "Static edges (`add_edge(\"a\", \"b\")`) are deterministic.\n",
        "**Conditional Edges** allow the graph to choose the next step dynamically based on the state.\n",
        "\n",
        "We will implement a **Router** that analyzes the user's intent.\n",
        "\n",
        "* If the user asks for code  Go to Coder.\n",
        "* If the user asks for concepts  Go to Researcher.\n"
      ],
      "metadata": {
        "id": "k2Cu0dJxG-Vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define the Router Logic\n",
        "def intent_router(state: ResearchState) -> Literal[\"search_node\", \"coder_node\"]:\n",
        "    \"\"\"\n",
        "    Classifies the user's intent to route to the correct node.\n",
        "    \"\"\"\n",
        "    last_message = state[\"messages\"][-1].content\n",
        "\n",
        "    # We ask the LLM to classify\n",
        "    classification_prompt = f\"\"\"\n",
        "    Classify the following user request into one of two categories: 'CODING' or 'RESEARCH'.\n",
        "\n",
        "    Request: {last_message}\n",
        "\n",
        "    Return ONLY the word 'CODING' or 'RESEARCH'.\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(classification_prompt).content.strip().upper()\n",
        "\n",
        "    if \"CODING\" in response:\n",
        "        print(\"üîÄ Router: Detected Coding Request -> Routing to Coder.\")\n",
        "        return \"coder_node\"\n",
        "    else:\n",
        "        print(\"üîÄ Router: Detected Research Request -> Routing to Search.\")\n",
        "        return \"search_node\"\n",
        "\n",
        "# 2. Define a dummy Coder Node for the router target\n",
        "def coder_node(state: ResearchState):\n",
        "    print(\"üíª Coder Agent: Generating Python code snippet...\")\n",
        "    return {\"draft_report\": f\"```python\\n# Code for {state['query']}\\nprint('Hello World')\\n```\"}\n",
        "\n",
        "print(\"‚úÖ Router and Coder Node defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkiA_lVvFTQ4",
        "outputId": "64b41ff6-73f5-4cf4-97b0-815bd51dd106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Router and Coder Node defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Compilation & Execution ‚öôÔ∏è\n",
        "\n",
        "### **Theory: The StateGraph**\n",
        "\n",
        "We now assemble the pieces.\n",
        "\n",
        "1. Initialize `StateGraph(ResearchState)`.\n",
        "2. Add Nodes.\n",
        "3. Add **Conditional Edges** using the router.\n",
        "4. `compile()` the graph into a `Runnable`.\n"
      ],
      "metadata": {
        "id": "bxoLwxRtHS31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workflow_routing = StateGraph(ResearchState)\n",
        "\n",
        "# Add Nodes\n",
        "workflow_routing.add_node(\"search_node\", search_node)\n",
        "workflow_routing.add_node(\"curator_node\", curator_node)\n",
        "workflow_routing.add_node(\"coder_node\", coder_node)\n",
        "\n",
        "# Add conditional edges from START using intent_router\n",
        "workflow_routing.add_conditional_edges(\n",
        "    START,\n",
        "    intent_router,\n",
        "    {\n",
        "        \"search_node\": \"search_node\",\n",
        "        \"coder_node\": \"coder_node\",\n",
        "    }\n",
        ")\n",
        "\n",
        "# Let's attach the Curator after search\n",
        "workflow_routing.add_edge(\"search_node\", \"curator_node\")\n",
        "workflow_routing.add_edge(\"curator_node\", END)\n",
        "workflow_routing.add_edge(\"coder_node\", END)\n",
        "\n",
        "# Compile\n",
        "app_routing = workflow_routing.compile()\n",
        "\n",
        "# Visualize\n",
        "print(app_routing.get_graph().draw_ascii())\n",
        "\n",
        "# Test Execution\n",
        "print(\"\\n--- üß™ Execution Test ---\")\n",
        "inputs = {\n",
        "    \"query\": \"LangGraph state management\",\n",
        "    \"messages\": [HumanMessage(content=\"Explain LangGraph state.\")]\n",
        "}\n",
        "result = app_routing.invoke(inputs)\n",
        "print(f\"\\nüìÑ Final Report:\\n{result['draft_report']}\")\n",
        "\n",
        "print(\"\\n--- üß™ Execution Test (Coding Request) ---\")\n",
        "inputs_code = {\n",
        "    \"query\": \"Python code for factorial\",\n",
        "    \"messages\": [HumanMessage(content=\"Give me Python code for calculating factorial.\")]\n",
        "}\n",
        "result_code = app_routing.invoke(inputs_code)\n",
        "print(f\"\\nüìÑ Final Report (Code):\\n{result_code['draft_report']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhmXOXLLFcXH",
        "outputId": "286c72fd-0d09-4462-cc32-4402e99ab7cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              +-----------+               \n",
            "              | __start__ |               \n",
            "              +-----------+               \n",
            "              ..           ..             \n",
            "            ..               ..           \n",
            "          ..                   ..         \n",
            "+-------------+                  ..       \n",
            "| search_node |                   .       \n",
            "+-------------+                   .       \n",
            "        *                         .       \n",
            "        *                         .       \n",
            "        *                         .       \n",
            "+--------------+           +------------+ \n",
            "| curator_node |           | coder_node | \n",
            "+--------------+           +------------+ \n",
            "              **           **             \n",
            "                **       **               \n",
            "                  **   **                 \n",
            "                +---------+               \n",
            "                | __end__ |               \n",
            "                +---------+               \n",
            "\n",
            "--- üß™ Execution Test ---\n",
            "üîÄ Router: Detected Coding Request -> Routing to Coder.\n",
            "üíª Coder Agent: Generating Python code snippet...\n",
            "\n",
            "üìÑ Final Report:\n",
            "```python\n",
            "# Code for LangGraph state management\n",
            "print('Hello World')\n",
            "```\n",
            "\n",
            "--- üß™ Execution Test (Coding Request) ---\n",
            "üîÄ Router: Detected Coding Request -> Routing to Coder.\n",
            "üíª Coder Agent: Generating Python code snippet...\n",
            "\n",
            "üìÑ Final Report (Code):\n",
            "```python\n",
            "# Code for Python code for factorial\n",
            "print('Hello World')\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 6: Loops & Self-Correction üîÑ\n",
        "\n",
        "### **Theory: Reflection**\n",
        "\n",
        "One-shot generation often leads to hallucinations or poor quality.\n",
        "**Agentic Workflow Pattern:** .\n",
        "\n",
        "We will add a **Critique Node**.\n",
        "\n",
        "1. LLM reviews the `draft_report`.\n",
        "2. If the quality is low, it returns `REJECT` and we loop back to the generator.\n",
        "3. If high, it returns `APPROVE` and we go to END.\n",
        "\n",
        "*Crucial:* We must track `critique_count` in our state to prevent infinite loops!\n"
      ],
      "metadata": {
        "id": "rMOcaahdHck1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def critique_node(state: ResearchState):\n",
        "    \"\"\"\n",
        "    Reviews the draft report.\n",
        "    \"\"\"\n",
        "    print(\"üßê Critic Agent: Reviewing draft...\")\n",
        "    current_draft = state[\"draft_report\"]\n",
        "    current_count = state.get(\"critique_count\", 0)\n",
        "\n",
        "    # Safety Valve: Prevent infinite loops\n",
        "    if current_count >= 3:\n",
        "        print(\"‚ö†Ô∏è Critic: Max retries reached. Approving anyway.\")\n",
        "        # The should_continue function will handle the END transition based on this count\n",
        "        return {\"critique_count\": current_count + 1}\n",
        "\n",
        "    # Ask LLM to critique\n",
        "    prompt = f\"\"\"\n",
        "    Review this draft for technical accuracy, brevity, and completeness:\n",
        "    {current_draft}\n",
        "\n",
        "    If the draft is excellent and requires no further improvements, say 'APPROVE'.\n",
        "    Otherwise, provide specific, concise feedback on how to improve it.\n",
        "    \"\"\"\n",
        "    response = llm.invoke(prompt).content\n",
        "\n",
        "    # We store the critique as a message to give context to the writer next time\n",
        "    return {\n",
        "        \"critique_count\": current_count + 1,\n",
        "        \"messages\": [AIMessage(content=f\"Critique: {response}\")]\n",
        "    }\n",
        "\n",
        "def should_continue(state: ResearchState) -> Literal[\"curator_node\", \"end_node\"]:\n",
        "    \"\"\"\n",
        "    Decides whether to loop back or end based on the last message.\n",
        "    \"\"\"\n",
        "    last_message_content = state[\"messages\"][-1].content\n",
        "\n",
        "    # Check if the critique explicitly approved or if max retries are met\n",
        "    if \"APPROVE\" in last_message_content or state[\"critique_count\"] > 2:\n",
        "        print(\"‚úÖ Critic: Quality standard met.\")\n",
        "        return \"end_node\"\n",
        "    else:\n",
        "        print(\"üîô Critic: Rejection. Looping back to Curator.\")\n",
        "        return \"curator_node\"\n",
        "\n",
        "# Update Graph with Cycle\n",
        "workflow_reflection = StateGraph(ResearchState)\n",
        "\n",
        "workflow_reflection.add_node(\"search_node\", search_node)\n",
        "workflow_reflection.add_node(\"curator_node\", curator_node)\n",
        "workflow_reflection.add_node(\"critique_node\", critique_node)\n",
        "\n",
        "workflow_reflection.add_edge(START, \"search_node\")\n",
        "workflow_reflection.add_edge(\"search_node\", \"curator_node\")\n",
        "workflow_reflection.add_edge(\"curator_node\", \"critique_node\")\n",
        "\n",
        "# The Conditional Edge creates the Cycle\n",
        "workflow_reflection.add_conditional_edges(\n",
        "    \"critique_node\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"end_node\": END,\n",
        "        \"curator_node\": \"curator_node\"\n",
        "    }\n",
        ")\n",
        "\n",
        "app_reflection = workflow_reflection.compile()\n",
        "print(app_reflection.get_graph().draw_ascii())\n",
        "\n",
        "# --- Small Test for Reflection Graph ---\n",
        "print(\"\\n--- üß™ Reflection Graph Test ---\")\n",
        "test_input_reflection = {\n",
        "    \"query\": \"Quantum Computing\",\n",
        "    \"messages\": [HumanMessage(content=\"Generate a short report on quantum computing.\")],\n",
        "    \"critique_count\": 0\n",
        "}\n",
        "\n",
        "final_reflection_output = app_reflection.invoke(test_input_reflection)\n",
        "print(f\"\\nüìù Final Report (after critique loop):\\n{final_reflection_output['draft_report']}\")\n",
        "print(f\"Total critiques: {final_reflection_output['critique_count']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLcy1ptSFpOm",
        "outputId": "17e812a0-d37e-4c1c-f07a-7c9bd76a3aa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  +-----------+    \n",
            "  | __start__ |    \n",
            "  +-----------+    \n",
            "        *          \n",
            "        *          \n",
            "        *          \n",
            " +-------------+   \n",
            " | search_node |   \n",
            " +-------------+   \n",
            "        *          \n",
            "        *          \n",
            "        *          \n",
            "+--------------+   \n",
            "| curator_node |   \n",
            "+--------------+   \n",
            "        *          \n",
            "        *          \n",
            "        *          \n",
            "+---------------+  \n",
            "| critique_node |  \n",
            "+---------------+  \n",
            "        .          \n",
            "        .          \n",
            "        .          \n",
            "   +---------+     \n",
            "   | __end__ |     \n",
            "   +---------+     \n",
            "\n",
            "--- üß™ Reflection Graph Test ---\n",
            "üåê Search Agent: Searching for 'Quantum Computing'...\n",
            "üß† Curator Agent: Synthesizing information...\n",
            "üßê Critic Agent: Reviewing draft...\n",
            "‚úÖ Critic: Quality standard met.\n",
            "\n",
            "üìù Final Report (after critique loop):\n",
            "As a senior technical writer, I've reviewed the provided document outlines on Quantum Computing. Here's a concise summary of their key contributions:\n",
            "\n",
            "---\n",
            "\n",
            "### Summary of Quantum Computing Documents\n",
            "\n",
            "This summary synthesizes key insights from three distinct documents pertaining to Quantum Computing, covering its fundamental theory, practical implementation, and common challenges.\n",
            "\n",
            "**Doc A: Technical Overview of Quantum Computing**\n",
            "Document A provides a comprehensive technical overview of Quantum Computing (QC). It delves into the foundational principles that distinguish QC from classical computation, such as qubits, superposition, entanglement, and quantum gates. The document likely explains how these principles are leveraged in the development of quantum algorithms (e.g., Shor's, Grover's) and outlines potential transformative applications across various fields, including cryptography, materials science, and drug discovery. Its primary focus is on establishing a solid theoretical understanding of the underlying physics and computational models.\n",
            "\n",
            "**Doc B: Implementation Details for Quantum Computing using Python**\n",
            "Document B focuses on the practical implementation of Quantum Computing using Python. It details how developers can leverage popular quantum programming frameworks, such as Qiskit or Cirq, to design, simulate, and execute quantum circuits. The content would cover the step-by-step process of defining qubits, applying quantum gates, managing quantum registers, and interpreting measurement results. It likely provides practical code examples, best practices, and guidance on setting up development environments for building quantum applications within a Python ecosystem.\n",
            "\n",
            "**Doc C: Common Pitfalls When Using Quantum Computing**\n",
            "Document C addresses common pitfalls and challenges encountered when working with Quantum Computing. This includes common misconceptions about achieving \"quantum advantage,\" difficulties in debugging complex quantum circuits, and managing the inherent noise and error rates of current quantum hardware (NISQ devices). The document likely offers guidance on proper problem framing, resource optimization, avoiding common programming errors, and setting realistic expectations regarding the capabilities and limitations of present-day quantum systems, emphasizing the need for a nuanced approach to development and application.\n",
            "\n",
            "---\n",
            "\n",
            "**Overall Synthesis:**\n",
            "Collectively, these documents provide a holistic view of Quantum Computing: from the theoretical underpinnings and practical development methodologies to critical considerations for successful and realistic application. They equip readers with both the \"what\" and \"why\" of QC, the \"how\" to implement it, and the crucial \"what to watch out for\" to navigate its complexities effectively.\n",
            "Total critiques: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 7: Tools & Actions üõ†Ô∏è\n",
        "\n",
        "### **Theory: ToolNode**\n",
        "\n",
        "LLMs can't do math or access real-time data natively. We bind **Tools** to the LLM.\n",
        "When the LLM decides to call a tool, it outputs a tool call request. A `ToolNode` executes this request and feeds the result back.\n",
        "\n",
        "We will integrate `Wikipedia` and a `Calculator`.\n"
      ],
      "metadata": {
        "id": "_QGF0RbFHqiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "\n",
        "# 1. Define Tools\n",
        "@tool\n",
        "def calculator(expression: str) -> str:\n",
        "    \"\"\"Calculates a mathematical expression.\"\"\"\n",
        "    try:\n",
        "        return f\"{expression} = {eval(expression)}\"\n",
        "    except:\n",
        "        return \"Invalid syntax\"\n",
        "\n",
        "# Setup Wikipedia Tool\n",
        "wiki = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
        "\n",
        "tools = [calculator, wiki]\n",
        "\n",
        "# 2. Bind Tools to LLM\n",
        "# This tells the LLM \"You have these functions available\"\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "# 3. Create a Node that uses the Tool-bound LLM\n",
        "def agent_node(state: ResearchState):\n",
        "    print(\"ü§ñ Agent: Reasoning...\")\n",
        "    messages = state[\"messages\"]\n",
        "    response = llm_with_tools.invoke(messages)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "# 4. Create the ToolNode (Pre-built by LangGraph to execute tool calls)\n",
        "tool_node = ToolNode(tools)\n",
        "\n",
        "# 5. Build Graph\n",
        "tool_graph = StateGraph(ResearchState)\n",
        "tool_graph.add_node(\"agent\", agent_node)\n",
        "tool_graph.add_node(\"tools\", tool_node)\n",
        "\n",
        "tool_graph.add_edge(START, \"agent\")\n",
        "\n",
        "# conditional_edges logic:\n",
        "# If LLM response has tool_calls -> Go to 'tools'\n",
        "# If LLM response is text -> Go to END\n",
        "tool_graph.add_conditional_edges(\"agent\", tools_condition)\n",
        "tool_graph.add_edge(\"tools\", \"agent\") # Loop back to agent to read tool output\n",
        "\n",
        "app_tools = tool_graph.compile()\n",
        "\n",
        "print(\"‚úÖ Tool Graph built.\")\n",
        "\n",
        "# --- Small Test for Tool Graph ---\n",
        "print(\"\\n--- üß™ Tool Graph Test ---\")\n",
        "\n",
        "# Test 1: Calculator tool\n",
        "print(\"\\n--- Testing Calculator Tool ---\")\n",
        "calculator_input = {\n",
        "    \"messages\": [HumanMessage(content=\"What is 123 + 456?\")]\n",
        "}\n",
        "calculator_result = app_tools.invoke(calculator_input)\n",
        "print(f\"Calculator Result: {calculator_result['messages'][-1].content}\")\n",
        "\n",
        "# Test 2: Wikipedia tool\n",
        "print(\"\\n--- Testing Wikipedia Tool ---\")\n",
        "wikipedia_input = {\n",
        "    \"messages\": [HumanMessage(content=\"Who is the current CEO of Google?\")]\n",
        "}\n",
        "wikipedia_result = app_tools.invoke(wikipedia_input)\n",
        "print(f\"Wikipedia Result: {wikipedia_result['messages'][-1].content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufxqVYGwFwiV",
        "outputId": "428d247d-e3a8-4feb-83d2-01c5ed813dda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Tool Graph built.\n",
            "\n",
            "--- üß™ Tool Graph Test ---\n",
            "\n",
            "--- Testing Calculator Tool ---\n",
            "ü§ñ Agent: Reasoning...\n",
            "ü§ñ Agent: Reasoning...\n",
            "Calculator Result: The sum of 123 and 456 is 579.\n",
            "\n",
            "--- Testing Wikipedia Tool ---\n",
            "ü§ñ Agent: Reasoning...\n",
            "ü§ñ Agent: Reasoning...\n",
            "Wikipedia Result: [{'type': 'text', 'text': 'Sundar Pichai is the current CEO of Google.', 'extras': {'signature': 'CtcEAXLI2nwjAlm6VBuYJHz4Hca6V+Br1s831iUq5Hj033Y+ZvLhqOKIkEG21T83KO37uXfRJ/dvvR6s6LDuI5yIgTst36AvZrRMDH57JA22ucV1BcOveZzSHf+iAZmlHl6X0BL/B9Xe2ZU1AyP3/UV60Q/5TjTtVs+kVq+T2OjTv5Nh15BLj/OkCJsk+I4HhohfR5lRnrMUiPZn9AH27gxZ/fwCzeW6kw+z2TlUy32S2nNqqG+OagqPj5XiGnb6lUUbvjK70vY2bgAbd3d36ro148oG+tX4n76ndGvhEpCjg0pJvvMgOTJgkj/Z/NUEwL4ifDxLthjtGlK6wMidkTNlE80sa5uadoKsO8l/ZtfX9WyWCa8u7u6lA8Qj1Tfwbu5Xffhma+HpDPyMSBGRkeTOYrsYFMylNL8z4f5/vkSipniv3vfv1GaQuq0gWA2WqjX//N7q5IstxISSAgb2WpVb+Go2F4U6m0wuCX4gmpwYloMesAPWbmJY+UbTKgN/cshKTZCCBg/QgqOChhV3ayQk5I7Pqdhx9nr8pA/EpDdYYn7xDv3sg9lzZlfChN9ZaOsqssWdYyYt/7QoUQoyVhnxIL8dgD38gYIvpawCAqEL++NZUNB4vcnPA2/o9hpkeWGQjBGXE9z1nFJ07K4MFgU4ufScmTmUQtbcx1h1Y4sywoFd9wbgBbGCNdLv7kfmXNYtDZIfzBizMKhU9mujjczGAlTd+/EHLumTjpFt0bcbOWKOj5q3xS8Alel3wfVsfQbi1b9WWzzZtcjfVHWskTGy3wj0OogkgLM='}}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 8: Time Travel with Checkpointers üï∞Ô∏è\n",
        "\n",
        "### **Theory: Persistence**\n",
        "\n",
        "Normally, when a script ends, memory is lost.\n",
        "**Checkpointers** save the `State` to a database (or in-memory for testing) at every step.\n",
        "This allows:\n",
        "\n",
        "1. **Human-in-the-loop:** Pause, wait for human approval, then resume.\n",
        "2. **Error Recovery:** Retry from the last known good state.\n"
      ],
      "metadata": {
        "id": "YTPtjOjjIVOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# 1. Initialize Memory\n",
        "memory = MemorySaver()\n",
        "\n",
        "# 2. Compile with Checkpointer\n",
        "# We reuse our reflection graph from Part 6\n",
        "app_persistent = workflow_reflection.compile(checkpointer=memory)\n",
        "\n",
        "# 3. Define a Thread ID (Like a session ID)\n",
        "thread_config = {\"configurable\": {\"thread_id\": \"session_1\"}}\n",
        "\n",
        "# 4. Run Step 1 - Initial Research\n",
        "print(\"--- üü¢ Run 1: Initial Research (Streaming Events) ---\")\n",
        "input_1 = {\n",
        "    \"query\": \"LangGraph Persistence\",\n",
        "    \"messages\": [HumanMessage(content=\"Research persistence in LangGraph.\")],\n",
        "    \"critique_count\": 0 # Reset critique count for a fresh run\n",
        "}\n",
        "\n",
        "print(\"\\n--- Events from Run 1 ---\")\n",
        "for s in app_persistent.stream(input_1, thread_config):\n",
        "    print(s)\n",
        "    # After each step, inspect the state to show persistence\n",
        "    current_state = app_persistent.get_state(thread_config)\n",
        "    print(f\"  üì∏ Current State after step: {current_state.values.get('query')}, Critiques: {current_state.values.get('critique_count')}\")\n",
        "\n",
        "state_snapshot_1 = app_persistent.get_state(thread_config)\n",
        "print(f\"\\nüì∏ Final Snapshot of State after Run 1: Query='{state_snapshot_1.values['query']}', Critiques='{state_snapshot_1.values['critique_count']}'\")\n",
        "\n",
        "\n",
        "# 5. Resume / Continue Conversation (Run 2) - This uses the saved state from Run 1\n",
        "print(\"\\n--- üü¢ Run 2: Follow up (Retains History and State) ---\")\n",
        "input_2 = {\"messages\": [HumanMessage(content=\"Summarize the persistence findings in 5 words.\")]}\n",
        "\n",
        "print(\"\\n--- Events from Run 2 ---\")\n",
        "# Because we pass the same thread_id, it remembers the previous context and state\n",
        "for s in app_persistent.stream(input_2, thread_config):\n",
        "    print(s)\n",
        "    current_state = app_persistent.get_state(thread_config)\n",
        "    print(f\"  üì∏ Current State after step: {current_state.values.get('query')}, Critiques: {current_state.values.get('critique_count')}\")\n",
        "\n",
        "final_state_after_run2 = app_persistent.get_state(thread_config)\n",
        "print(f\"\\nüìù Final Report (after follow-up):\\n{final_state_after_run2.values['draft_report']}\")\n",
        "print(f\"Total critiques after follow-up: {final_state_after_run2.values['critique_count']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJIPHITuGPqu",
        "outputId": "c94622e2-1f9c-4c4c-afa6-8f45026cd6c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üü¢ Run 1: Initial Research (Streaming Events) ---\n",
            "\n",
            "--- Events from Run 1 ---\n",
            "üåê Search Agent: Searching for 'LangGraph Persistence'...\n",
            "{'search_node': {'documents': ['Doc A: Technical overview of LangGraph Persistence.', 'Doc B: Implementation details for LangGraph Persistence using Python.', 'Doc C: Common pitfalls when using LangGraph Persistence.']}}\n",
            "  üì∏ Current State after step: LangGraph Persistence, Critiques: 0\n",
            "üß† Curator Agent: Synthesizing information...\n",
            "{'curator_node': {'draft_report': 'As a senior technical writer, I\\'ve reviewed the hypothetical content of these three documents concerning LangGraph Persistence. Here is a consolidated summary:\\n\\n---\\n\\n### LangGraph Persistence: A Comprehensive Overview\\n\\nLangGraph Persistence is a fundamental capability designed to maintain and manage the state of complex, multi-step agentic workflows within the LangGraph framework. It enables agents to remember conversational context, intermediate reasoning steps, and tool interactions across multiple invocations, user sessions, or even system restarts. This ensures continuity, reliability, and an improved user experience for long-running or interactive AI applications.\\n\\n**Doc A: Technical Overview of LangGraph Persistence**\\n\\nDoc A positions LangGraph Persistence as the core mechanism for overcoming the stateless nature of traditional API calls in AI applications. It explains the \"why\" behind persistence, highlighting its critical role in:\\n\\n*   **Maintaining Conversational State:** Allowing agents to recall past interactions and build on previous turns without re-processing all history.\\n*   **Enabling Long-Running Workflows:** Supporting complex reasoning chains, human-in-the-loop processes, or multi-stage tasks that span extended periods.\\n*   **Resilience and Fault Tolerance:** Recovering agent state after interruptions, system failures, or application restarts, preventing data loss and enhancing reliability.\\n*   **Efficient Resource Utilization:** Avoiding redundant computations by storing intermediate results and accessible graph states.\\n\\nThe document likely describes LangGraph\\'s state model, which captures not just raw messages but the complete internal state of the graph, including node visits, variable values, and any custom data defined within the graph structure. It introduces the concept of a `CheckpointSaver` as the abstract interface responsible for saving and loading this state, emphasizing its role in decoupling the persistence mechanism from the core graph logic.\\n\\n**Doc B: Implementation Details for LangGraph Persistence using Python**\\n\\nDoc B dives into the practical \"how-to\" of implementing LangGraph Persistence for Python developers. It details the concrete steps and API usage for integrating state management into LangGraph applications:\\n\\n*   **`CheckpointSaver` Implementations:** The document would showcase various concrete `CheckpointSaver` classes available in LangGraph, such as:\\n    *   `SqliteSaver`: For local, lightweight, single-process persistence (e.g., during development or for single-user desktop applications).\\n    *   `RedisSaver`: For robust, distributed, and scalable persistence, suitable for production environments requiring high availability and concurrency.\\n    *   Potentially other database-specific savers (e.g., PostgreSQL, MongoDB) or custom implementations demonstrating how to extend the `CheckpointSaver` interface.\\n*   **Integration with the Graph:** It illustrates how to instantiate a chosen `CheckpointSaver` and attach it to a LangGraph `Graph` or `AgentExecutor` instance using methods like `with_checkpoint()`.\\n*   **`thread_id` Usage:** A crucial aspect highlighted would be the use of a unique `thread_id` (or similar identifier) when invoking the graph. This `thread_id` serves as the key to load and save the correct conversational state, ensuring that different user interactions or separate workflows maintain their isolation.\\n*   **Configuration and Customization:** Details on connection strings, database parameters, and potentially strategies for managing multiple checkpoints within a single database.\\n*   **State Management Operations:** While `with_checkpoint` simplifies most use cases, it might also touch upon lower-level APIs for explicit state retrieval (`get_state`) or manual updates, offering finer control for advanced scenarios.\\n\\n**Doc C: Common Pitfalls when using LangGraph Persistence**\\n\\nDoc C provides crucial guidance on avoiding common errors and challenges that can arise when deploying LangGraph Persistence, ensuring robust and reliable applications. Key pitfalls and their remedies include:\\n\\n*   **Incorrect `CheckpointSaver` Selection:** Using `SqliteSaver` in a multi-user, production environment can lead to performance bottlenecks, deadlocks, and data corruption.\\n    *   **Recommendation:** Always select a distributed, robust `CheckpointSaver` (e.g., `RedisSaver` or a database-backed solution) for concurrent, production-grade applications.\\n*   **Missing or Inconsistent `thread_id`:** Failing to provide a unique `thread_id` or reusing `thread_id`s incorrectly will lead to state mixing, data loss, or agents getting confused about their conversational context.\\n    *   **Recommendation:** Generate and manage unique `thread_id`s for each user session or distinct workflow, ensuring consistency across invocations.\\n*   **State Bloat and Performance Degradation:** Storing excessively large states, especially with long-running conversations, can lead to increased latency for persistence operations and higher storage costs.\\n    *   **Recommendation:** Optimize the data stored in the graph\\'s state, consider strategies for summarizing or pruning older history if not strictly necessary for future turns, and evaluate the performance characteristics of the chosen `CheckpointSaver`.\\n*   **Security Concerns with Sensitive Data:** Persisting sensitive user data or API keys within the graph state without proper encryption or access controls.\\n    *   **Recommendation:** Implement data encryption at rest and in transit, ensure strong access controls for the persistence layer, and avoid storing highly sensitive information directly if possible.\\n*   **Schema Evolution Challenges:** Changes to the LangGraph\\'s internal state structure or custom data models can lead to compatibility issues when trying to load older checkpoints.\\n    *   **Recommendation:** Plan for versioning strategies, implement data migration routines for older checkpoints, or design state models with backward compatibility in mind.\\n*   **Concurrency Issues:** Competing writes to the same checkpoint, especially with custom or poorly implemented `CheckpointSaver`s, can lead to data races or inconsistent states.\\n    *   **Recommendation:** Leverage `CheckpointSaver`s that inherently handle concurrency (e.g., atomic operations in Redis, transactional databases) or implement explicit locking/optimistic concurrency mechanisms.\\n*   **Inadequate Error Handling:** Not gracefully handling failures in persistence operations (e.g., database connection issues) can lead to silent data loss or application crashes.\\n    *   **Recommendation:** Implement robust error handling, logging, and retry mechanisms for persistence operations.\\n\\n---\\n\\nIn conclusion, these three documents provide a holistic understanding of LangGraph Persistence. Doc A establishes the architectural rationale, Doc B offers the developer-centric implementation guide, and Doc C serves as a crucial best-practice and troubleshooting manual, collectively enabling developers to build resilient, stateful, and production-ready LangGraph applications.', 'messages': [AIMessage(content='As a senior technical writer, I\\'ve reviewed the hypothetical content of these three documents concerning LangGraph Persistence. Here is a consolidated summary:\\n\\n---\\n\\n### LangGraph Persistence: A Comprehensive Overview\\n\\nLangGraph Persistence is a fundamental capability designed to maintain and manage the state of complex, multi-step agentic workflows within the LangGraph framework. It enables agents to remember conversational context, intermediate reasoning steps, and tool interactions across multiple invocations, user sessions, or even system restarts. This ensures continuity, reliability, and an improved user experience for long-running or interactive AI applications.\\n\\n**Doc A: Technical Overview of LangGraph Persistence**\\n\\nDoc A positions LangGraph Persistence as the core mechanism for overcoming the stateless nature of traditional API calls in AI applications. It explains the \"why\" behind persistence, highlighting its critical role in:\\n\\n*   **Maintaining Conversational State:** Allowing agents to recall past interactions and build on previous turns without re-processing all history.\\n*   **Enabling Long-Running Workflows:** Supporting complex reasoning chains, human-in-the-loop processes, or multi-stage tasks that span extended periods.\\n*   **Resilience and Fault Tolerance:** Recovering agent state after interruptions, system failures, or application restarts, preventing data loss and enhancing reliability.\\n*   **Efficient Resource Utilization:** Avoiding redundant computations by storing intermediate results and accessible graph states.\\n\\nThe document likely describes LangGraph\\'s state model, which captures not just raw messages but the complete internal state of the graph, including node visits, variable values, and any custom data defined within the graph structure. It introduces the concept of a `CheckpointSaver` as the abstract interface responsible for saving and loading this state, emphasizing its role in decoupling the persistence mechanism from the core graph logic.\\n\\n**Doc B: Implementation Details for LangGraph Persistence using Python**\\n\\nDoc B dives into the practical \"how-to\" of implementing LangGraph Persistence for Python developers. It details the concrete steps and API usage for integrating state management into LangGraph applications:\\n\\n*   **`CheckpointSaver` Implementations:** The document would showcase various concrete `CheckpointSaver` classes available in LangGraph, such as:\\n    *   `SqliteSaver`: For local, lightweight, single-process persistence (e.g., during development or for single-user desktop applications).\\n    *   `RedisSaver`: For robust, distributed, and scalable persistence, suitable for production environments requiring high availability and concurrency.\\n    *   Potentially other database-specific savers (e.g., PostgreSQL, MongoDB) or custom implementations demonstrating how to extend the `CheckpointSaver` interface.\\n*   **Integration with the Graph:** It illustrates how to instantiate a chosen `CheckpointSaver` and attach it to a LangGraph `Graph` or `AgentExecutor` instance using methods like `with_checkpoint()`.\\n*   **`thread_id` Usage:** A crucial aspect highlighted would be the use of a unique `thread_id` (or similar identifier) when invoking the graph. This `thread_id` serves as the key to load and save the correct conversational state, ensuring that different user interactions or separate workflows maintain their isolation.\\n*   **Configuration and Customization:** Details on connection strings, database parameters, and potentially strategies for managing multiple checkpoints within a single database.\\n*   **State Management Operations:** While `with_checkpoint` simplifies most use cases, it might also touch upon lower-level APIs for explicit state retrieval (`get_state`) or manual updates, offering finer control for advanced scenarios.\\n\\n**Doc C: Common Pitfalls when using LangGraph Persistence**\\n\\nDoc C provides crucial guidance on avoiding common errors and challenges that can arise when deploying LangGraph Persistence, ensuring robust and reliable applications. Key pitfalls and their remedies include:\\n\\n*   **Incorrect `CheckpointSaver` Selection:** Using `SqliteSaver` in a multi-user, production environment can lead to performance bottlenecks, deadlocks, and data corruption.\\n    *   **Recommendation:** Always select a distributed, robust `CheckpointSaver` (e.g., `RedisSaver` or a database-backed solution) for concurrent, production-grade applications.\\n*   **Missing or Inconsistent `thread_id`:** Failing to provide a unique `thread_id` or reusing `thread_id`s incorrectly will lead to state mixing, data loss, or agents getting confused about their conversational context.\\n    *   **Recommendation:** Generate and manage unique `thread_id`s for each user session or distinct workflow, ensuring consistency across invocations.\\n*   **State Bloat and Performance Degradation:** Storing excessively large states, especially with long-running conversations, can lead to increased latency for persistence operations and higher storage costs.\\n    *   **Recommendation:** Optimize the data stored in the graph\\'s state, consider strategies for summarizing or pruning older history if not strictly necessary for future turns, and evaluate the performance characteristics of the chosen `CheckpointSaver`.\\n*   **Security Concerns with Sensitive Data:** Persisting sensitive user data or API keys within the graph state without proper encryption or access controls.\\n    *   **Recommendation:** Implement data encryption at rest and in transit, ensure strong access controls for the persistence layer, and avoid storing highly sensitive information directly if possible.\\n*   **Schema Evolution Challenges:** Changes to the LangGraph\\'s internal state structure or custom data models can lead to compatibility issues when trying to load older checkpoints.\\n    *   **Recommendation:** Plan for versioning strategies, implement data migration routines for older checkpoints, or design state models with backward compatibility in mind.\\n*   **Concurrency Issues:** Competing writes to the same checkpoint, especially with custom or poorly implemented `CheckpointSaver`s, can lead to data races or inconsistent states.\\n    *   **Recommendation:** Leverage `CheckpointSaver`s that inherently handle concurrency (e.g., atomic operations in Redis, transactional databases) or implement explicit locking/optimistic concurrency mechanisms.\\n*   **Inadequate Error Handling:** Not gracefully handling failures in persistence operations (e.g., database connection issues) can lead to silent data loss or application crashes.\\n    *   **Recommendation:** Implement robust error handling, logging, and retry mechanisms for persistence operations.\\n\\n---\\n\\nIn conclusion, these three documents provide a holistic understanding of LangGraph Persistence. Doc A establishes the architectural rationale, Doc B offers the developer-centric implementation guide, and Doc C serves as a crucial best-practice and troubleshooting manual, collectively enabling developers to build resilient, stateful, and production-ready LangGraph applications.', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019b7d9d-931a-73e3-9d40-198c4aed3b0c-0', usage_metadata={'input_tokens': 65, 'output_tokens': 3102, 'total_tokens': 3167, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1753}})]}}\n",
            "  üì∏ Current State after step: LangGraph Persistence, Critiques: 0\n",
            "üßê Critic Agent: Reviewing draft...\n",
            "‚úÖ Critic: Quality standard met.\n",
            "{'critique_node': {'critique_count': 1, 'messages': [AIMessage(content='Critique: APPROVE', additional_kwargs={}, response_metadata={})]}}\n",
            "  üì∏ Current State after step: LangGraph Persistence, Critiques: 1\n",
            "\n",
            "üì∏ Final Snapshot of State after Run 1: Query='LangGraph Persistence', Critiques='1'\n",
            "\n",
            "--- üü¢ Run 2: Follow up (Retains History and State) ---\n",
            "\n",
            "--- Events from Run 2 ---\n",
            "üåê Search Agent: Searching for 'LangGraph Persistence'...\n",
            "{'search_node': {'documents': ['Doc A: Technical overview of LangGraph Persistence.', 'Doc B: Implementation details for LangGraph Persistence using Python.', 'Doc C: Common pitfalls when using LangGraph Persistence.']}}\n",
            "  üì∏ Current State after step: LangGraph Persistence, Critiques: 1\n",
            "üß† Curator Agent: Synthesizing information...\n",
            "{'curator_node': {'draft_report': 'As a senior technical writer, here\\'s a summary of the provided documents regarding LangGraph Persistence, synthesizing their key insights for a comprehensive understanding:\\n\\n---\\n\\n## LangGraph Persistence: A Comprehensive Overview\\n\\nLangGraph Persistence provides the critical capability to save and restore the internal state of a running LangGraph application. This is fundamental for building robust, long-running agentic workflows that can recover from interruptions, resume execution across sessions, and operate effectively in distributed environments.\\n\\n### Doc A: Technical Overview ‚Äì The \"What\" and \"Why\"\\n\\nDoc A establishes LangGraph Persistence as the mechanism for ensuring **resilience** and **recoverability** in multi-step AI applications. Its core concept is **checkpointing**, which involves periodically saving the entire internal state of a graph. This state encompasses all channel values, the history of node executions, and any other relevant internal data structures.\\n\\nThe \"why\" is crucial:\\n*   **Failure Recovery:** Allows an interrupted agent workflow to resume from its last saved state, preventing loss of progress.\\n*   **Long-Running Processes:** Enables agents to maintain their \"memory\" and context over extended periods, even if the underlying process restarts.\\n*   **Distributed Execution:** Facilitates the sharing and management of agent state across multiple instances or services.\\n*   **Human-in-the-Loop:** Supports scenarios where human intervention occurs, allowing the system to pause and later resume with all context intact.\\n\\nIn essence, persistence ensures that the \"thread of execution\" and the agent\\'s \"understanding\" of its ongoing task are never truly lost.\\n\\n### Doc B: Implementation Details ‚Äì The \"How\" (in Python)\\n\\nDoc B details the practical implementation of LangGraph Persistence in Python. The primary interface is the `StateManager` abstraction, specifically through concrete `Checkpointer` implementations.\\n\\nKey implementation aspects include:\\n\\n1.  **Checkpointer Integration:** Users typically integrate a `Checkpointer` when compiling their LangGraph application. For example:\\n    ```python\\n    from langgraph.checkpoint import SqliteSaver, RedisSaver\\n    # ... define your graph ...\\n    checkpointer = SqliteSaver(conn_string=\"sqlite:///path/to/my_db.sqlite\") # For local development\\n    # OR\\n    # checkpointer = RedisSaver(redis_url=\"redis://localhost:6379/0\") # For production\\n    app = graph.compile(checkpointer=checkpointer)\\n    ```\\n2.  **`thread_id` Management:** State is uniquely identified and managed per `thread_id`. When invoking the graph, this ID dictates which state to load or save.\\n    ```python\\n    # Resume or start a new conversation for a specific user\\n    config = {\"configurable\": {\"thread_id\": \"user-123\"}}\\n    app.invoke({\"input\": \"Hello\"}, config)\\n    ```\\n3.  **Automatic State Management:** For common operations like `app.invoke()` or `app.stream()`, the `Checkpointer` automatically handles loading the latest state for the given `thread_id` before execution and saving the updated state afterwards.\\n4.  **Direct State Access:** For more advanced use cases, the `Checkpointer` allows direct access to state via methods like `get_state()` and `update_state()`, enabling fine-grained control or inspection.\\n5.  **Available Implementations:** Common built-in `Checkpointer` options include:\\n    *   `SqliteSaver`: Excellent for local development and simple, single-process applications.\\n    *   `RedisSaver`: Ideal for production environments, offering robust, distributed, and scalable persistence.\\n\\n### Doc C: Common Pitfalls ‚Äì The \"Beware Of\"\\n\\nDoc C provides crucial warnings and best practices to avoid common issues when using LangGraph Persistence. A senior technical writer would emphasize these for robust system design:\\n\\n1.  **Performance Overheads:**\\n    *   **Serialization/Deserialization:** Large or complex states can incur significant CPU and memory overheads when being converted to/from storage formats. Optimize state size.\\n    *   **I/O Latency:** Disk I/O (for `SqliteSaver`) or network latency (for `RedisSaver` or other remote stores) can impact overall agent response times. Choose appropriate storage solutions and consider caching strategies.\\n\\n2.  **Data Consistency and Concurrency:**\\n    *   **Race Conditions:** In distributed or highly concurrent environments, multiple processes attempting to update the same `thread_id` concurrently can lead to lost updates or corrupted state. Implement optimistic locking, distributed locks, or design operations to be idempotent.\\n    *   **Schema Evolution:** Changes to the structure of your custom state objects can break deserialization of older checkpoints. Plan for schema migration strategies, versioning, or robust error handling during deserialization.\\n\\n3.  **Serialization Challenges:**\\n    *   **Non-Serializable Objects:** Ensure all custom objects stored within your agent\\'s state are properly serializable (e.g., using `pydantic` models or implementing custom `__dict__` or `__getstate__` methods). Errors here can be subtle.\\n    *   **Complex Types:** Some complex Python objects might require custom serialization logic to be correctly persisted and rehydrated.\\n\\n4.  **Resource Management:**\\n    *   **Connection Pooling:** For database-backed savers, ensure proper connection pooling to manage database connections efficiently and prevent resource exhaustion.\\n    *   **State Cleanup:** Over time, old `thread_id` states can accumulate, consuming storage. Implement a strategy for archiving or deleting stale checkpoints.\\n\\n5.  **Debugging Complexity:**\\n    *   Persisted state can be complex to inspect directly, especially if it\\'s stored in a compressed or binary format. Develop tools or methods for visualizing and debugging saved states.\\n\\n### Conclusion\\n\\nLangGraph Persistence is a powerful feature, essential for building reliable and scalable AI agents. By understanding its technical underpinnings (Doc A), mastering its implementation (Doc B), and proactively addressing potential pitfalls (Doc C), developers can ensure their LangGraph applications are resilient, efficient, and maintainable. Careful design, especially concerning state structure, concurrency, and performance, is paramount for success.', 'messages': [AIMessage(content='As a senior technical writer, here\\'s a summary of the provided documents regarding LangGraph Persistence, synthesizing their key insights for a comprehensive understanding:\\n\\n---\\n\\n## LangGraph Persistence: A Comprehensive Overview\\n\\nLangGraph Persistence provides the critical capability to save and restore the internal state of a running LangGraph application. This is fundamental for building robust, long-running agentic workflows that can recover from interruptions, resume execution across sessions, and operate effectively in distributed environments.\\n\\n### Doc A: Technical Overview ‚Äì The \"What\" and \"Why\"\\n\\nDoc A establishes LangGraph Persistence as the mechanism for ensuring **resilience** and **recoverability** in multi-step AI applications. Its core concept is **checkpointing**, which involves periodically saving the entire internal state of a graph. This state encompasses all channel values, the history of node executions, and any other relevant internal data structures.\\n\\nThe \"why\" is crucial:\\n*   **Failure Recovery:** Allows an interrupted agent workflow to resume from its last saved state, preventing loss of progress.\\n*   **Long-Running Processes:** Enables agents to maintain their \"memory\" and context over extended periods, even if the underlying process restarts.\\n*   **Distributed Execution:** Facilitates the sharing and management of agent state across multiple instances or services.\\n*   **Human-in-the-Loop:** Supports scenarios where human intervention occurs, allowing the system to pause and later resume with all context intact.\\n\\nIn essence, persistence ensures that the \"thread of execution\" and the agent\\'s \"understanding\" of its ongoing task are never truly lost.\\n\\n### Doc B: Implementation Details ‚Äì The \"How\" (in Python)\\n\\nDoc B details the practical implementation of LangGraph Persistence in Python. The primary interface is the `StateManager` abstraction, specifically through concrete `Checkpointer` implementations.\\n\\nKey implementation aspects include:\\n\\n1.  **Checkpointer Integration:** Users typically integrate a `Checkpointer` when compiling their LangGraph application. For example:\\n    ```python\\n    from langgraph.checkpoint import SqliteSaver, RedisSaver\\n    # ... define your graph ...\\n    checkpointer = SqliteSaver(conn_string=\"sqlite:///path/to/my_db.sqlite\") # For local development\\n    # OR\\n    # checkpointer = RedisSaver(redis_url=\"redis://localhost:6379/0\") # For production\\n    app = graph.compile(checkpointer=checkpointer)\\n    ```\\n2.  **`thread_id` Management:** State is uniquely identified and managed per `thread_id`. When invoking the graph, this ID dictates which state to load or save.\\n    ```python\\n    # Resume or start a new conversation for a specific user\\n    config = {\"configurable\": {\"thread_id\": \"user-123\"}}\\n    app.invoke({\"input\": \"Hello\"}, config)\\n    ```\\n3.  **Automatic State Management:** For common operations like `app.invoke()` or `app.stream()`, the `Checkpointer` automatically handles loading the latest state for the given `thread_id` before execution and saving the updated state afterwards.\\n4.  **Direct State Access:** For more advanced use cases, the `Checkpointer` allows direct access to state via methods like `get_state()` and `update_state()`, enabling fine-grained control or inspection.\\n5.  **Available Implementations:** Common built-in `Checkpointer` options include:\\n    *   `SqliteSaver`: Excellent for local development and simple, single-process applications.\\n    *   `RedisSaver`: Ideal for production environments, offering robust, distributed, and scalable persistence.\\n\\n### Doc C: Common Pitfalls ‚Äì The \"Beware Of\"\\n\\nDoc C provides crucial warnings and best practices to avoid common issues when using LangGraph Persistence. A senior technical writer would emphasize these for robust system design:\\n\\n1.  **Performance Overheads:**\\n    *   **Serialization/Deserialization:** Large or complex states can incur significant CPU and memory overheads when being converted to/from storage formats. Optimize state size.\\n    *   **I/O Latency:** Disk I/O (for `SqliteSaver`) or network latency (for `RedisSaver` or other remote stores) can impact overall agent response times. Choose appropriate storage solutions and consider caching strategies.\\n\\n2.  **Data Consistency and Concurrency:**\\n    *   **Race Conditions:** In distributed or highly concurrent environments, multiple processes attempting to update the same `thread_id` concurrently can lead to lost updates or corrupted state. Implement optimistic locking, distributed locks, or design operations to be idempotent.\\n    *   **Schema Evolution:** Changes to the structure of your custom state objects can break deserialization of older checkpoints. Plan for schema migration strategies, versioning, or robust error handling during deserialization.\\n\\n3.  **Serialization Challenges:**\\n    *   **Non-Serializable Objects:** Ensure all custom objects stored within your agent\\'s state are properly serializable (e.g., using `pydantic` models or implementing custom `__dict__` or `__getstate__` methods). Errors here can be subtle.\\n    *   **Complex Types:** Some complex Python objects might require custom serialization logic to be correctly persisted and rehydrated.\\n\\n4.  **Resource Management:**\\n    *   **Connection Pooling:** For database-backed savers, ensure proper connection pooling to manage database connections efficiently and prevent resource exhaustion.\\n    *   **State Cleanup:** Over time, old `thread_id` states can accumulate, consuming storage. Implement a strategy for archiving or deleting stale checkpoints.\\n\\n5.  **Debugging Complexity:**\\n    *   Persisted state can be complex to inspect directly, especially if it\\'s stored in a compressed or binary format. Develop tools or methods for visualizing and debugging saved states.\\n\\n### Conclusion\\n\\nLangGraph Persistence is a powerful feature, essential for building reliable and scalable AI agents. By understanding its technical underpinnings (Doc A), mastering its implementation (Doc B), and proactively addressing potential pitfalls (Doc C), developers can ensure their LangGraph applications are resilient, efficient, and maintainable. Careful design, especially concerning state structure, concurrency, and performance, is paramount for success.', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019b7d9d-e9b1-7640-a085-8529e8cbe3a0-0', usage_metadata={'input_tokens': 65, 'output_tokens': 2866, 'total_tokens': 2931, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1564}})]}}\n",
            "  üì∏ Current State after step: LangGraph Persistence, Critiques: 1\n",
            "üßê Critic Agent: Reviewing draft...\n",
            "‚úÖ Critic: Quality standard met.\n",
            "{'critique_node': {'critique_count': 2, 'messages': [AIMessage(content='Critique: APPROVE', additional_kwargs={}, response_metadata={})]}}\n",
            "  üì∏ Current State after step: LangGraph Persistence, Critiques: 2\n",
            "\n",
            "üìù Final Report (after follow-up):\n",
            "As a senior technical writer, here's a summary of the provided documents regarding LangGraph Persistence, synthesizing their key insights for a comprehensive understanding:\n",
            "\n",
            "---\n",
            "\n",
            "## LangGraph Persistence: A Comprehensive Overview\n",
            "\n",
            "LangGraph Persistence provides the critical capability to save and restore the internal state of a running LangGraph application. This is fundamental for building robust, long-running agentic workflows that can recover from interruptions, resume execution across sessions, and operate effectively in distributed environments.\n",
            "\n",
            "### Doc A: Technical Overview ‚Äì The \"What\" and \"Why\"\n",
            "\n",
            "Doc A establishes LangGraph Persistence as the mechanism for ensuring **resilience** and **recoverability** in multi-step AI applications. Its core concept is **checkpointing**, which involves periodically saving the entire internal state of a graph. This state encompasses all channel values, the history of node executions, and any other relevant internal data structures.\n",
            "\n",
            "The \"why\" is crucial:\n",
            "*   **Failure Recovery:** Allows an interrupted agent workflow to resume from its last saved state, preventing loss of progress.\n",
            "*   **Long-Running Processes:** Enables agents to maintain their \"memory\" and context over extended periods, even if the underlying process restarts.\n",
            "*   **Distributed Execution:** Facilitates the sharing and management of agent state across multiple instances or services.\n",
            "*   **Human-in-the-Loop:** Supports scenarios where human intervention occurs, allowing the system to pause and later resume with all context intact.\n",
            "\n",
            "In essence, persistence ensures that the \"thread of execution\" and the agent's \"understanding\" of its ongoing task are never truly lost.\n",
            "\n",
            "### Doc B: Implementation Details ‚Äì The \"How\" (in Python)\n",
            "\n",
            "Doc B details the practical implementation of LangGraph Persistence in Python. The primary interface is the `StateManager` abstraction, specifically through concrete `Checkpointer` implementations.\n",
            "\n",
            "Key implementation aspects include:\n",
            "\n",
            "1.  **Checkpointer Integration:** Users typically integrate a `Checkpointer` when compiling their LangGraph application. For example:\n",
            "    ```python\n",
            "    from langgraph.checkpoint import SqliteSaver, RedisSaver\n",
            "    # ... define your graph ...\n",
            "    checkpointer = SqliteSaver(conn_string=\"sqlite:///path/to/my_db.sqlite\") # For local development\n",
            "    # OR\n",
            "    # checkpointer = RedisSaver(redis_url=\"redis://localhost:6379/0\") # For production\n",
            "    app = graph.compile(checkpointer=checkpointer)\n",
            "    ```\n",
            "2.  **`thread_id` Management:** State is uniquely identified and managed per `thread_id`. When invoking the graph, this ID dictates which state to load or save.\n",
            "    ```python\n",
            "    # Resume or start a new conversation for a specific user\n",
            "    config = {\"configurable\": {\"thread_id\": \"user-123\"}}\n",
            "    app.invoke({\"input\": \"Hello\"}, config)\n",
            "    ```\n",
            "3.  **Automatic State Management:** For common operations like `app.invoke()` or `app.stream()`, the `Checkpointer` automatically handles loading the latest state for the given `thread_id` before execution and saving the updated state afterwards.\n",
            "4.  **Direct State Access:** For more advanced use cases, the `Checkpointer` allows direct access to state via methods like `get_state()` and `update_state()`, enabling fine-grained control or inspection.\n",
            "5.  **Available Implementations:** Common built-in `Checkpointer` options include:\n",
            "    *   `SqliteSaver`: Excellent for local development and simple, single-process applications.\n",
            "    *   `RedisSaver`: Ideal for production environments, offering robust, distributed, and scalable persistence.\n",
            "\n",
            "### Doc C: Common Pitfalls ‚Äì The \"Beware Of\"\n",
            "\n",
            "Doc C provides crucial warnings and best practices to avoid common issues when using LangGraph Persistence. A senior technical writer would emphasize these for robust system design:\n",
            "\n",
            "1.  **Performance Overheads:**\n",
            "    *   **Serialization/Deserialization:** Large or complex states can incur significant CPU and memory overheads when being converted to/from storage formats. Optimize state size.\n",
            "    *   **I/O Latency:** Disk I/O (for `SqliteSaver`) or network latency (for `RedisSaver` or other remote stores) can impact overall agent response times. Choose appropriate storage solutions and consider caching strategies.\n",
            "\n",
            "2.  **Data Consistency and Concurrency:**\n",
            "    *   **Race Conditions:** In distributed or highly concurrent environments, multiple processes attempting to update the same `thread_id` concurrently can lead to lost updates or corrupted state. Implement optimistic locking, distributed locks, or design operations to be idempotent.\n",
            "    *   **Schema Evolution:** Changes to the structure of your custom state objects can break deserialization of older checkpoints. Plan for schema migration strategies, versioning, or robust error handling during deserialization.\n",
            "\n",
            "3.  **Serialization Challenges:**\n",
            "    *   **Non-Serializable Objects:** Ensure all custom objects stored within your agent's state are properly serializable (e.g., using `pydantic` models or implementing custom `__dict__` or `__getstate__` methods). Errors here can be subtle.\n",
            "    *   **Complex Types:** Some complex Python objects might require custom serialization logic to be correctly persisted and rehydrated.\n",
            "\n",
            "4.  **Resource Management:**\n",
            "    *   **Connection Pooling:** For database-backed savers, ensure proper connection pooling to manage database connections efficiently and prevent resource exhaustion.\n",
            "    *   **State Cleanup:** Over time, old `thread_id` states can accumulate, consuming storage. Implement a strategy for archiving or deleting stale checkpoints.\n",
            "\n",
            "5.  **Debugging Complexity:**\n",
            "    *   Persisted state can be complex to inspect directly, especially if it's stored in a compressed or binary format. Develop tools or methods for visualizing and debugging saved states.\n",
            "\n",
            "### Conclusion\n",
            "\n",
            "LangGraph Persistence is a powerful feature, essential for building reliable and scalable AI agents. By understanding its technical underpinnings (Doc A), mastering its implementation (Doc B), and proactively addressing potential pitfalls (Doc C), developers can ensure their LangGraph applications are resilient, efficient, and maintainable. Careful design, especially concerning state structure, concurrency, and performance, is paramount for success.\n",
            "Total critiques after follow-up: 2\n"
          ]
        }
      ]
    }
  ]
}