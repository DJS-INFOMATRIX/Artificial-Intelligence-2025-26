{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42015794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain sentence-transformers faiss-cpu langchain-community langchain-openai transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4966cc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade --quiet langchain langchain-community sentence-transformers faiss-cpu transformers|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad329e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas langchain-text-splitters sentence-transformers numpy #Installation now complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "398ac92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install weaviate-client sentence-transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "437c1f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41c06b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "43e57a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install \"weaviate-client>=3.26.0,<4.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd017fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install huggingface_hub[hf_xet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "71a25ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from transformers import pipeline\n",
    "from langchain_community.document_loaders import CSVLoader, DirectoryLoader, PyPDFLoader\n",
    "import typing\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2e41f238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from langchain_community.vectorstores import Weaviate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ee51ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f93538a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7863eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #loader to lead the data into document structure\n",
    "# loader = DirectoryLoader(path = 'data',glob = '**/*.csv',loader_cls= CSVLoader,show_progress = True)\n",
    "# #splitter for chunking the docs\n",
    "# splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd10e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ingestion:\n",
    "    def __init__(self,model_name: str = \"all-MiniLM-L6-v2\",k:int =5): \n",
    "                    #  loader:DirectoryLoader = loader, \n",
    "                    #  splitter: RecursiveCharacterTextSplitter = splitter):\n",
    "                \n",
    "        #Loading the documents:\n",
    "        self.loader = DirectoryLoader(path = 'data',glob = '**/*.pdf',loader_cls= PyPDFLoader,show_progress = True)\n",
    "        self.docs = self.loader.load()\n",
    "        #chunking\n",
    "        self.splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 100)\n",
    "        self.chunks = self.splitter.split_documents(self.docs)\n",
    "        #Embeddings\n",
    "        self.model_name = model_name\n",
    "        self.embedding_generator = SentenceTransformerEmbeddings(model_name = self.model_name)\n",
    "        #Crceating vectorstore \n",
    "        self.vectorstore = self.initialize_weaviate()\n",
    "        # self.initialize_weaviate()\n",
    "        \n",
    "        self.retriever = self.vectorstore.as_retriever(search_kwargs = {'k':k})\n",
    "        \n",
    "\n",
    "\n",
    "    #initializing weaviate\n",
    "    def initialize_weaviate(self):\n",
    "        try:\n",
    "            # self.client = weaviate.connect_to_local(\n",
    "            #      host = 'localhost',\n",
    "            #      port = 8080\n",
    "            # )\n",
    "\n",
    "            self.client = weaviate.Client(\n",
    "                    url=\"http://localhost:8080\",\n",
    "                    timeout_config=(5, 15)  # (connect timeout, read timeout)\n",
    "                )\n",
    "\n",
    "            # we first need to <docker compose up -d> weaviate  \n",
    "            if not self.client.is_ready():\n",
    "                    raise ConnectionError(\"Weaviate is not ready. Ensure Docker container is running: docker compose up -d\")\n",
    "\n",
    "            print(\"Connected to Weaviate\")\n",
    "\n",
    "            return Weaviate(\n",
    "                client = self.client,\n",
    "                index_name = 'Chunk',\n",
    "                text_key = 'text',\n",
    "                embedding = self.embedding_generator,\n",
    "                by_text = False \n",
    "            )\n",
    "        except Exception as e:\n",
    "             raise ConnectionError(f\"Failed to connect to Weaviate: {e}\\nMake sure to run: docker compose up -d\")\n",
    "            # print(f\"Error: {e}\\n Failed to connect to weaviate\") \n",
    "\n",
    "    #Uploading the generated embeddings into weaviate:-\n",
    "\n",
    "    def upload_vectors(self, skip_if_exists: bool = True):\n",
    "        try:\n",
    "            if skip_if_exists:\n",
    "                # Check if the Chunk class exists first\n",
    "                try:\n",
    "                    schema = self.client.schema.get()\n",
    "                    class_names = [c['class'] for c in schema.get('classes', [])]\n",
    "                    \n",
    "                    if 'Chunk' in class_names:\n",
    "                        # Class exists, check count\n",
    "                        result = self.client.query.aggregate(\"Chunk\").with_meta_count().do()\n",
    "                        existing_count = result.get('data', {}).get('Aggregate', {}).get('Chunk', [{}])[0].get('meta', {}).get('count', 0)\n",
    "                        \n",
    "                        if existing_count > 0:\n",
    "                            print(f\"⚠ Weaviate already contains {existing_count} chunks. Skipping upload.\")\n",
    "                            print(\"To force re-upload, delete the class first or set skip_if_exists=False\")\n",
    "                            return\n",
    "                    else:\n",
    "                        print(\"ℹ 'Chunk' class doesn't exist yet. Creating and uploading...\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"ℹ Could not check existing data: {e}. Proceeding with upload...\")\n",
    "\n",
    "            # Upload documents - LangChain will create the schema automatically\n",
    "            print(f\"Uploading {len(self.chunks)} chunks to Weaviate...\")\n",
    "            self.vectorstore.add_documents(self.chunks)\n",
    "            print(f\"✓ Successfully uploaded {len(self.chunks)} chunks to Weaviate\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error uploading vectors: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    # def upload_vectors(self, skip_if_exists: bool = True):\n",
    "\n",
    "    #     if skip_if_exists:\n",
    "    #             result = self.client.query.aggregate(\"Chunk\").with_meta_count().do()\n",
    "    #             existing_count = result.get('data', {}).get('Aggregate', {}).get('Chunk', [{}])[0].get('meta', {}).get('count', 0)\n",
    "                \n",
    "    #             if existing_count > 0:\n",
    "    #                 print(f\"⚠ Weaviate already contains {existing_count} chunks. Skipping upload.\")\n",
    "    #                 print(\"To force re-upload, delete the class first or set skip_if_exists=False\")\n",
    "    #                 return\n",
    "\n",
    "    #     self.vectorstore.add_documents(self.chunks)\n",
    "    #     #Langchain weaviate needs only this one line of code for adding chunks\n",
    "    #     print(f\"Successfully uploaded {len(self.chunks)} into Weaviate\")\n",
    "\n",
    "    def delete_all_vectors(self, class_name = 'Chunk'):\n",
    "        \"\"\"Delete all vectors from Weaviate by deleting the Chunk class\"\"\"\n",
    "        try:\n",
    "            schema = self.client.schema.get()\n",
    "            class_names = [c['class'] for c in schema.get('classes', [])]\n",
    "            \n",
    "            if class_name in class_names:\n",
    "                self.client.schema.delete_class(class_name)\n",
    "                print(f\"✓ Deleted all vectors ({class_name} class removed)\")\n",
    "            else:\n",
    "                print(f\"ℹ No {class_name} class found. Nothing to delete.\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error deleting vectors: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d70cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ingestion():\n",
    "    ingest = Ingestion()\n",
    "    # ingest.initialize_weaviate()\n",
    "    ingest.delete_all_vectors()\n",
    "    ingest.upload_vectors()\n",
    "# ingestion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f559961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM():\n",
    "    def __init__(self,prompt,llm_name = 'microsoft/Phi-3-mini-4k-instruct'):#meta-llama/Llama-3.2-3B-Instruct# google/gemma-2b\n",
    "        self.llm_name = llm_name\n",
    "        self.prompt = prompt\n",
    "        self.response = None\n",
    "        try:\n",
    "            print(f\"Loading Model:{self.llm_name}... This may take some time \")\n",
    "            \n",
    "            self.llm_model = pipeline(\n",
    "                'text-generation', \n",
    "                model = self.llm_name,\n",
    "                # device_map = 'auto',\n",
    "                max_length = 2048,\n",
    "                )\n",
    "            \n",
    "            print(\"Generating response...\")\n",
    "\n",
    "            result = self.llm_model(\n",
    "                self.prompt, max_new_tokens = 200 , \n",
    "                num_return_sequences = 1, \n",
    "                do_sample = True, \n",
    "                temperature = 0.6, \n",
    "                top_p = 0.9,\n",
    "                return_full_text=True,\n",
    "                pad_token_id=50256,\n",
    "                truncation=True\n",
    "                )\n",
    "            \n",
    "            full_text = result[0]['generated_text']\n",
    "            # Remove the prompt part\n",
    "            self.response = full_text[len(self.prompt):].strip()\n",
    "            print(\"Response Generated Successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'error: {e}')\n",
    "            self.response = f\"Failed to generate a response.(Error: {e})\"\n",
    "    \n",
    "    def answer_query(self):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"ANSWER:\\n{self.response}\")\n",
    "        print(f\"{'='*50}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "70d76171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query(Ingestion):\n",
    "    def __init__(self):\n",
    "        # Creating the retriver\n",
    "        super().__init__()\n",
    "        # self.retriever = self.vectorstore.as_retriever(search_kwargs = {'k':k})\n",
    "        self.query = None\n",
    "        # self.context = self.get_context()\n",
    "        self.context = None \n",
    "        # self.prompt = self.generate_prompt()\n",
    "        self.prompt = None\n",
    "    \n",
    "    def get_query(self):\n",
    "        try:\n",
    "            inp = input(\"Enter any cricket related question(ask it in short): \")\n",
    "            if inp.strip():\n",
    "                self.query = inp\n",
    "            else:\n",
    "                raise ValueError(\"Empty Input\")\n",
    "        except Exception as e:\n",
    "            self.query = \"What is the importance of toss in Cricket\"\n",
    "            print(f\"Error: {e}\\n\\n Therefore, using default query\")\n",
    "\n",
    "    def get_context(self):\n",
    "        retrieved_docs = self.retriever.invoke(self.query)# newer version sue get_relevant_documents\n",
    "        context = \"\\n\\n\".join([d.page_content for d in retrieved_docs])\n",
    "        self.context = context\n",
    "    \n",
    "    def generate_prompt(self):\n",
    "        self.prompt =  f\"\"\"You are a cricket expert. Answer the question on the basis of the context below.\n",
    "        Respond in point format if multiple lines are there. Format the answer properly.\n",
    "        Context:{self.context}\n",
    "        Question:{self.query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "    \n",
    "    def generate_response(self):\n",
    "        llm = LLM(self.prompt)\n",
    "        llm.answer_query()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fb64cc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_rag(query: Query):\n",
    "    query.get_query()   #It takes the user Query\n",
    "    query.get_context() #It generates the context for the Specific query\n",
    "    query.generate_prompt()\n",
    "    query.generate_response()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "27012699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:19<00:00,  9.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Weaviate\n"
     ]
    }
   ],
   "source": [
    "query = Query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bdc49191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model:microsoft/Phi-3-mini-4k-instruct... This may take some time \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.21it/s]\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating response...\n",
      "Response Generated Successfully\n",
      "\n",
      "==================================================\n",
      "ANSWER:\n",
      "- A ball is called a no ball if it is delivered in breach of clause 20.4.2.7 (41.4 -\tDeliberate attempt to distract striker before playing the ball or 41.5 -\tDeliberate attempt of Distraction, Deception or Obstruction of striker after the stroke).\n",
      "        - A ball is called a no ball to over-ride a wide ball.\n",
      "        - If a fielder's thrown ball makes contact with a camera on or over the field of play, the umpire shall call and signal a dead ball.\n",
      "        - If a no ball is called following a check by the third umpire, the batting side shall benefit from the reversal of the dismissal and the one run for the No ball, but shall not benefit from any runs that may have accrued from the delivery had the on-field umpire originally\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask_rag(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28216327",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
